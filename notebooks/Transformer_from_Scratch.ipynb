{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer Model Notebooks**\n",
        "> Code ini ditulis berdasarkan paper : https://arxiv.org/pdf/1706.03762 (***Attetion is All You Need***)"
      ],
      "metadata": {
        "id": "My52rs0XpB4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Library**\n",
        "> Mengimport pustaka yang digunakan selama proses eksperimen"
      ],
      "metadata": {
        "id": "RY2vLMUEipL3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfoG3vOjiLKp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, LayerNormalization, Dense, Dropout, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Multi-head Attention**\n",
        "> Layer Multi-Head Attention adalah komponen kunci dari model Transformer"
      ],
      "metadata": {
        "id": "DgC3c7MkitSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = Dense(d_model)\n",
        "        self.wk = Dense(d_model)\n",
        "        self.wv = Dense(d_model)\n",
        "\n",
        "        self.dense = Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "metadata": {
        "id": "O8fnAy1dimy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Layer FFN (*Feed Forward Neural Network*)**\n",
        "> Layer *Feed-Forward Neural Network* adalah layer yang terdiri dari dua fully connected layers dengan aktivasi non-linear di antaranya."
      ],
      "metadata": {
        "id": "8hwU-76Vi1g9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(Layer):\n",
        "    def __init__(self, d_model, dff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.dense1 = Dense(dff, activation='relu')\n",
        "        self.dense2 = Dense(d_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "BFlpRQLmiyuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Encoder Layer**\n",
        "*Encoder layer* menggabungkan *Multi-Head Attention* dan *Feed-Forward Neural Network*, dengan Layer Normalization dan residual connections."
      ],
      "metadata": {
        "id": "rWWG6KNei9iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ],
      "metadata": {
        "id": "IW5al6QAi8PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Encoder**\n",
        "> Model Encoder terdiri dari beberapa layer Encoder yang ditumpuk"
      ],
      "metadata": {
        "id": "kjLgBPHgjEkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = self.positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "                                     tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "                                     d_model)\n",
        "\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angle_rates = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return pos * angle_rates"
      ],
      "metadata": {
        "id": "P1afytojjIoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Membuat Decoder Layer**\n",
        "> Decoder layer menggabungkan dua layer Multi-Head Attention (satu untuk self-attention dan satu untuk cross-attention) dan Feed-Forward Neural Network, dengan Layer Normalization dan residual connections"
      ],
      "metadata": {
        "id": "fRE6ij31mD6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "        self.dropout3 = Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "id": "Or59tIgwmMYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Membuat Model Decoder**\n",
        "> Model Decoder terdiri dari beberapa layer Decoder yang ditumpuk"
      ],
      "metadata": {
        "id": "Vt_VkB_UjNXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = self.positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "        return x, attention_weights\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "                                     tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "                                     d_model)\n",
        "\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angle_rates = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return pos * angle_rates"
      ],
      "metadata": {
        "id": "saJfcQqyjKot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Membuat model Transformer (Encoder-Decoder)**\n",
        "> Proses menyusun semua komponen menjadi model Transformer lengkap dengan encoder-decoder"
      ],
      "metadata": {
        "id": "dXq9tW2zmgSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size,\n",
        "                 pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inputs, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inputs[0], training, enc_padding_mask)\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            inputs[1], enc_output, training, look_ahead_mask, dec_padding_mask\n",
        "        )\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "metadata": {
        "id": "icp5QD_Smsit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Membuat Masking**\n",
        "> Masking digunakan untuk menghindari informasi dari token-token di masa depan saat melakukan self-attention"
      ],
      "metadata": {
        "id": "oXTOxmAzjZLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ],
      "metadata": {
        "id": "8KXvO20_jmBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Membuat Model dan Melatihnya**\n",
        "> Proses membuat instance dari model Transformer dan melatihnya dengan data yang sesuai."
      ],
      "metadata": {
        "id": "mkuZSafljrLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Fungsi untuk membuat padding mask\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "# Fungsi untuk membuat look-ahead mask\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (size, size)\n",
        "\n",
        "# Hyperparameter\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "dff = 512\n",
        "input_vocab_size = 8500\n",
        "target_vocab_size = 8000\n",
        "pe_input = 10000\n",
        "pe_target = 6000\n",
        "rate = 0.1\n",
        "\n",
        "# Dummy class Transformer (Ganti dengan implementasi sebenarnya)\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inputs, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        input_seq, target_seq = inputs\n",
        "        # Implementasi model transformer seharusnya di sini\n",
        "        x = tf.random.uniform((tf.shape(input_seq)[0], tf.shape(target_seq)[1], self.d_model))  # Simulasi output Transformer\n",
        "        return self.final_layer(x), None  # output, attention_weights\n",
        "\n",
        "# Membuat model\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate)\n",
        "\n",
        "# Contoh input\n",
        "input_seq = tf.random.uniform((64, 32), minval=0, maxval=input_vocab_size, dtype=tf.int32)\n",
        "target_seq = tf.random.uniform((64, 32), minval=0, maxval=target_vocab_size, dtype=tf.int32)\n",
        "\n",
        "# Membuat mask\n",
        "enc_padding_mask = create_padding_mask(input_seq)\n",
        "look_ahead_mask = create_look_ahead_mask(tf.shape(target_seq)[1])  # Panjang sequence target\n",
        "dec_padding_mask = create_padding_mask(input_seq)\n",
        "\n",
        "# Melakukan prediksi\n",
        "output, attention_weights = transformer(\n",
        "    [input_seq, target_seq],\n",
        "    training=True,\n",
        "    enc_padding_mask=enc_padding_mask,\n",
        "    look_ahead_mask=look_ahead_mask,\n",
        "    dec_padding_mask=dec_padding_mask\n",
        ")"
      ],
      "metadata": {
        "id": "eknrdVI9j1hp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}